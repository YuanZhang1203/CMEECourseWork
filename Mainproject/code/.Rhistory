y<-dat[,2]
# DEFINE THE ERROR TERM error.term<-(y-a-b*x)
# REMEMBER THE NORMAL pdf? density<-dnorm(error.term, mean=0, sd=sigma, log=T)
# THE LOG-LIKELIHOOD IS THE SUM OF INDIVIDUAL LOG-DENSITY return(sum(density))
}
regression.log.likelihood(c(1,1,1), dat=recapture.data)
optim(par=c(1,1,1), regression.log.likelihood, method='L-BFGS-B',
lower=c(-1000,-1000,0.0001), upper=c(1000,1000,10000), control=list(fnscale=-1), dat=recapture.data, hessian=T)
# DEFINE THE ERROR TERM error.term<-(y-a-b*x)
# REMEMBER THE NORMAL pdf? density<-dnorm(error.term, mean=0, sd=sigma, log=T)
# THE LOG-LIKELIHOOD IS THE SUM OF INDIVIDUAL LOG-DENSITY return(sum(density))
}
View(question_21)
# TO OPIMISE THE LOG-LIKELIHOOD FUNCTION IN R
# optimize() IS ONE-DIMENSIONAL,
# optim() GENERALISES TO MULTI-DIMENSIONAL CASES
optim(par=c(1,1,1), regression.log.likelihood, method='L-BFGS-B',
lower=c(-1000,-1000,0.0001), upper=c(1000,1000,10000), control=list(fnscale=-1), dat=recapture.data, hessian=T)
# JUST TO SEE WHAT THE LOG-LIKELIHOOD VALUE IS WHEN a=1, b=1, and sigma=1
# YOU MAY TRY ANY DIFFERENT VALUES
regression.log.likelihood(c(1,1,1), dat=recapture.data)
$par
# TO OPIMISE THE LOG-LIKELIHOOD FUNCTION IN R
# optimize() IS ONE-DIMENSIONAL,
# optim() GENERALISES TO MULTI-DIMENSIONAL CASES
optim(par=c(1,1,1), regression.log.likelihood, method='L-BFGS-B',
lower=c(-1000,-1000,0.0001), upper=c(1000,1000,10000), control=list(fnscale=-1), dat=recapture.data, hessian=T)
# THE LOG-LIKELIHOOD FOR THE LINEAR REGRESSION # PARAMETERS HAVE TO BE INPUT AS A VECTOR
regression.log.likelihood<-function(parm, dat)
{
# DEFINE THE PARAMETERS parm
# WE HAVE THREE PARAMETERS: a, b, sigma. BE CAREFUL OF THE ORDER
a<-parm[1]
b<-parm[2]
sigma<-parm[3]
# DEFINE THE DATA dat
# FIRST COLUMN IS x, SECOND COLUMN IS y
x<-dat[,1]
y<-dat[,2]
# DEFINE THE ERROR TERM
error.term<-(y-a-b*x)
# REMEMBER THE NORMAL pdf?
density<-dnorm(error.term, mean=0, sd=sigma, log=T)
# THE LOG-LIKELIHOOD IS THE SUM OF INDIVIDUAL LOG-DENSITY
return(sum(density))
}
$par
$par
regression.log.likelihood(c(1,1,1), dat=recapture.data)
optim(par=c(1,1,1), regression.log.likelihood, method='L-BFGS-B',
lower=c(-1000,-1000,0.0001), upper=c(1000,1000,10000), control=list(fnscale=-1), dat=recapture.data, hessian=T)
b<-seq(2, 4, 0.1)
sigma<-seq(2, 5, 0.1)
log.likelihood.value<-matrix(nr=length(b), nc=length(sigma))
install.packages("persp")
# COMPUTE THE LOG-LIKELIHOOD VALUE FOR EACH PAIR OF PARAMETERS
for (i in 1:length(b))
{
for (j in 1:length(sigma))
{
log.likelihood.value[i,j]<- regression.no.intercept.log.likelihood(parm=c(b[i],sigma[j]), dat=recapture.data)
}
}
# WE ARE INTERESTED IN KNOWING THE RELATIVE LOG-LIKELIHOOD VALUE # RELATIVE TO THE PEAK (MAXIMUM)
rel.log.likelihood.value<-log.likelihood.value-M1$value
# FUNCTION FOR 3D PLOT
persp(b, sigma, rel.log.likelihood.value, theta=30, phi=20,
xlab='b', ylab='sigma', zlab='rel.log.likelihood.value', col='grey')
# FUNCTION FOR 3D PLOT
persp(b, sigma, rel.log.likelihood.value, theta=30, phi=20,
xlab='b', ylab='sigma', zlab='rel.log.likelihood.value', col='grey')
# THE LOG-LIKELIHOOD FOR THE LINEAR REGRESSION # PARAMETERS HAVE TO BE INPUT AS A VECTOR
regression.log.likelihood<-function(parm, dat)
{
# DEFINE THE PARAMETERS parm
# WE HAVE THREE PARAMETERS: a, b, sigma. BE CAREFUL OF THE ORDER
a<-parm[1]
b<-parm[2]
sigma<-parm[3]
# DEFINE THE DATA dat
# FIRST COLUMN IS x, SECOND COLUMN IS y
x<-dat[,1]
y<-dat[,2]
# DEFINE THE ERROR TERM
error.term<-(y-a-b*x)
# REMEMBER THE NORMAL pdf?
density<-dnorm(error.term, mean=0, sd=sigma, log=T)
# THE LOG-LIKELIHOOD IS THE SUM OF INDIVIDUAL LOG-DENSITY
return(sum(density))
}
# JUST TO SEE WHAT THE LOG-LIKELIHOOD VALUE IS WHEN a=1, b=1, and sigma=1
# YOU MAY TRY ANY DIFFERENT VALUES
regression.log.likelihood(c(1,1,1), dat=recapture.data)
# READ IN DATASET
recapture.data<-read.csv('recapture.csv', header=T)
# DEFINE THE RANGE OF PARAMETERS TO BE PLOTTED
b<-seq(2, 4, 0.1)
sigma<-seq(2, 5, 0.1)
# THE LOG-LIKELIHOOD VALUE IS STORED IN A MATRIX
log.likelihood.value<-matrix(nr=length(b), nc=length(sigma))
# COMPUTE THE LOG-LIKELIHOOD VALUE FOR EACH PAIR OF PARAMETERS
for (i in 1:length(b))
{
for (j in 1:length(sigma))
{
log.likelihood.value[i,j]<- regression.no.intercept.log.likelihood(parm=c(b[i],sigma[j]), dat=recapture.data)
}
}
# WE ARE INTERESTED IN KNOWING THE RELATIVE LOG-LIKELIHOOD VALUE # RELATIVE TO THE PEAK (MAXIMUM)
rel.log.likelihood.value<-log.likelihood.value-M1$value
# FUNCTION FOR 3D PLOT
persp(b, sigma, rel.log.likelihood.value, theta=30, phi=20,
xlab='b', ylab='sigma', zlab='rel.log.likelihood.value', col='grey')
# READ IN DATASET
recapture.data<-read.csv('recapture.csv', header=T)
# SCATTERPLOT
plot(recapture.data$day, recapture.data$length_diff)
# THE LOG-LIKELIHOOD FOR THE LINEAR REGRESSION # PARAMETERS HAVE TO BE INPUT AS A VECTOR
regression.log.likelihood<-function(parm, dat)
{
# DEFINE THE PARAMETERS parm
# WE HAVE THREE PARAMETERS: a, b, sigma. BE CAREFUL OF THE ORDER
a<-parm[1]
b<-parm[2]
sigma<-parm[3]
# DEFINE THE DATA dat
# FIRST COLUMN IS x, SECOND COLUMN IS y
x<-dat[,1]
y<-dat[,2]
# DEFINE THE ERROR TERM
error.term<-(y-a-b*x)
# REMEMBER THE NORMAL pdf?
density<-dnorm(error.term, mean=0, sd=sigma, log=T)
# THE LOG-LIKELIHOOD IS THE SUM OF INDIVIDUAL LOG-DENSITY
return(sum(density))
}
# JUST TO SEE WHAT THE LOG-LIKELIHOOD VALUE IS WHEN a=1, b=1, and sigma=1
# YOU MAY TRY ANY DIFFERENT VALUES
regression.log.likelihood(c(1,1,1), dat=recapture.data)
# TO OPIMISE THE LOG-LIKELIHOOD FUNCTION IN R
# optimize() IS ONE-DIMENSIONAL,
# optim() GENERALISES TO MULTI-DIMENSIONAL CASES
optim(par=c(1,1,1), regression.log.likelihood, method='L-BFGS-B',
lower=c(-1000,-1000,0.0001), upper=c(1000,1000,10000), control=list(fnscale=-1), dat=recapture.data, hessian=T)
# DEFINE THE RANGE OF PARAMETERS TO BE PLOTTED
b<-seq(2, 4, 0.1)
sigma<-seq(2, 5, 0.1)
# THE LOG-LIKELIHOOD VALUE IS STORED IN A MATRIX
log.likelihood.value<-matrix(nr=length(b), nc=length(sigma))
# COMPUTE THE LOG-LIKELIHOOD VALUE FOR EACH PAIR OF PARAMETERS
for (i in 1:length(b))
{
for (j in 1:length(sigma))
{
log.likelihood.value[i,j]<- regression.no.intercept.log.likelihood(parm=c(b[i],sigma[j]), dat=recapture.data)
}
}
# WE ARE INTERESTED IN KNOWING THE RELATIVE LOG-LIKELIHOOD VALUE # RELATIVE TO THE PEAK (MAXIMUM)
rel.log.likelihood.value<-log.likelihood.value-M1$value
# FUNCTION FOR 3D PLOT
persp(b, sigma, rel.log.likelihood.value, theta=30, phi=20,
xlab='b', ylab='sigma', zlab='rel.log.likelihood.value', col='grey')
D
# DEFINE THE RANGE OF PARAMETERS TO BE PLOTTED
b<-seq(2, 4, 0.1)
persp(b, sigma, rel.log.likelihood.value, theta=30, phi=20,
xlab='b', ylab='sigma', zlab='rel.log.likelihood.value', col='grey')
M1<-optim(par=c(1,1), regression.no.intercept.log.likelihood,
dat=recapture.data, method='L-BFGS-B', lower=c(-1000,0.0001), upper=c(1000,10000), control=list(fnscale=-1), hessian=T)
M2<-optim(par=c(1,1,1), regression.log.likelihood, dat=recapture.data, method='L-BFGS-B', lower=c(-1000,-1000,0.0001), upper=c(1000,1000,10000), control=list(fnscale=-1), hessian=T)
View(M2)
View(log.likelihood.value)
M1<-optim(par=c(1,1), regression.no.intercept.log.likelihood,
dat=recapture.data, method='L-BFGS-B', lower=c(-1000,0.0001), upper=c(1000,10000), control=list(fnscale=-1), hessian=T)
####################################################Log-likelihood function for M1
# THE LOG-LIKELIHOOD FUNCTION FOR M1 WITHOUT AN INTERCEPT
regression.no.intercept.log.likelihood<-function(parm, dat)
{
# DEFINE THE PARAMETERS
# NO INTERCEPT THIS TIME
b<-parm[1]
sigma<-parm[2]
# DEFINE THE DATA # SAME AS BEFORE
x<-dat[,1]
y<-dat[,2]
# DEFINE THE ERROR TERM, NO INTERCEPT HERE
error.term<-(y-b*x)
# REMEMBER THE NORMAL pdf?
density<-dnorm(error.term, mean=0, sd=sigma, log=T)
# LOG-LIKELIHOOD IS THE SUM OF THE DENSITIES
return(sum(density))
}
# PERFORMING LIKELIHOOD-RATIO TEST
M1<-optim(par=c(1,1), regression.no.intercept.log.likelihood,
dat=recapture.data, method='L-BFGS-B', lower=c(-1000,0.0001), upper=c(1000,10000), control=list(fnscale=-1), hessian=T)
View(M1)
# DEFINE THE RANGE OF PARAMETERS TO BE PLOTTED
b<-seq(2, 4, 0.1)
sigma<-seq(2, 5, 0.1)
# THE LOG-LIKELIHOOD VALUE IS STORED IN A MATRIX
log.likelihood.value<-matrix(nr=length(b), nc=length(sigma))
# COMPUTE THE LOG-LIKELIHOOD VALUE FOR EACH PAIR OF PARAMETERS
for (i in 1:length(b))
{
for (j in 1:length(sigma))
{
log.likelihood.value[i,j]<- regression.no.intercept.log.likelihood(parm=c(b[i],sigma[j]), dat=recapture.data)
}
}
# WE ARE INTERESTED IN KNOWING THE RELATIVE LOG-LIKELIHOOD VALUE # RELATIVE TO THE PEAK (MAXIMUM)
rel.log.likelihood.value<-log.likelihood.value-M1$value
# FUNCTION FOR 3D PLOT
persp(b, sigma, rel.log.likelihood.value, theta=30, phi=20,
xlab='b', ylab='sigma', zlab='rel.log.likelihood.value', col='grey')
# CONTOUR PLOT
contour(b, sigma, rel.log.likelihood.value, xlab='b', ylab='sigma',
xlim=c(2.5, 3.9), ylim=c(2.0, 4.3),
levels=c(-1:-5, -10), cex=2)
# DRAW A CROSS TO INDICATE THE MAXIMUM
points(M1$par[1], M1$par[2], pch=3)
contour.line<-contourLines(b, sigma, rel.log.likelihood.value, levels=-1.92)[[1]]
lines(contour.line$x, contour.line$y, col='red',lty=2, lwd=2)
lines(contour.line$x, contour.line$y, col='red',lty=2, lwd=2)
result<-optim(par=c(1,1), regression.no.intercept.log.likelihood, method='L-BFGS-B',
lower=c(-1000,0.0001), upper=c(1000,10000),
control=list(fnscale=-1), dat=recapture.data, hessian=T)
# OBTAIN THE HESSIAN MATRIX
result$hessian
var.con.matrix<-(-1)*solve(result$hessian)
var.cov.matrix
View(var.con.matrix)
var.con.matrix
p<-seq(0.01, 0.99, 0.01)
p
log.likelihood.value<- coin.log.likelihood(p, n=50, y=35)
log.likelihood.value
plot(p, log.likelihood.value, type='l')
# THE VARIANCE-COVARIANCE MATRIX IS THE NEGATIVE OF # THE INVERSE OF THE HESSIAN MATRIX.
# BY solve() FUNCTION
var.cov.matrix<-(-1)*solve(result$hessian)
var.cov.matrix
############practical 4 13Feb 2020
coin.log.likelihood<-function(p, n, y)
{
return(lchoose(n, y) + y*log(p) + (n-y)*log(1-p))
}
p<-seq(0.01, 0.99, 0.01)
p
log.likelihood.value<- coin.log.likelihood(p, n=50, y=35)
log.likelihood.value
plot(p, log.likelihood.value, type='l')
plot(p, log.likelihood.value, type='l', xlim=(0.5, 0.9),y=(-10, 0))
plot(p, log.likelihood.value, type='l', xlim=(0.55, 0.85),y=(-4, -2))
############practical 4 13Feb 2020
coin.log.likelihood<-function(p, n, y)
{
return(lchoose(n, y) + y*log(p) + (n-y)*log(1-p))
}
p<-seq(0.01, 0.99, 0.01)
p
log.likelihood.value<- coin.log.likelihood(p, n=50, y=35)
log.likelihood.value
plot(p, log.likelihood.value, type='l')
plot(p, log.likelihood.value, type='l', xlim=(0.5, 0.9),y=(-10, 0))
plot(p, log.likelihood.value, type='l', xlim=c(0.5,0.9), ylim=c(-10, 0))
plot(p, log.likelihood.value, type='l', xlim=c(0.55, 0.85), ylim=c(-4, -2))
optimize(coin.log.likelihood, interval = c(0,1), maximum=T)
optimize(coin.log.likelihood, interval = c(0,1), n=50, y=35, maximum=T)
abline(h=-2.100895-1.92, col='red', lty=2)
plot(p, log.likelihood.value, type='l', xlim=c(0.55, 0.85), ylim=c(-4.1, -2))
abline(h=-2.100895-1.92, col='red', lty=2)
uniroot(function(p)(coin.log.likelihood(p=p, n=50, y=35)+2.1+1.92), interval = c(o, 0.7))
uniroot(function(p)(coin.log.likelihood(p=p, n=50, y=35)+2.1+1.92), interval = c(0, 0.7))
uniroot(function(p)(coin.log.likelihood(p=p, n=50, y=35)+2.1+1.92), interval = c(0.7, 1))
M2
load("/Users/changjiu/Downloads/WF.RData")
M<-WF(2)
M
M% * %M
M$ * $M
M% * %M
M%*%M
sample(c("h", "t",size=3, prob=c(0.5, 0.5)))
sample(c("h", "t),size=3, prob=c(0.5, 0.5))
)
)]
)
}
the_number_alleles_1 = 2 * N - 2 * N * p0
regression.log.likelihood<-function(parm, dat)
sample(c("h", "t"),size=3, prob=c(0.5, 0.5))
sample(c("h", "t"),size=3, prob=c(0.5, 0.5), "replace = True")
mu <- 2
tau <- 0.5
x <- seq(-4,10,0.01)
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,0.6),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3)
mu <- 2
tau <- 0.5
x <- seq(-4,10,0.01)
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,1.0),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3)
mu <- 2
tau <- 0.5
x <- seq(-4,10,0.01)
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,2.0),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3)
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,2.0),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3) # prior
setwd CMEECoursework
CMEECourseWork/Bayesian/Materials/Notebooks/
library(readr)
functions <- read_csv("CMEECourseWork/Bayesian/Materials/Notebooks/Math/Data/functions.R")
View(functions)
install.packages("fields")
source("CMEECourseWork/Bayesian/Materials/Notebooks/math/Data/functions.R")
alleles <- c('A', 'C', 'G', 'T')
like <- calcGenoLikes("AAGAGGA", "A", "G", 0.05, FALSE)
print(likes)
prior <- c(1/3,1/3,1/3)
num <-likes * prior
print(num)
post <- num/sum(num)
print(post)
likes <- calcGenoLikes("AAGAGGA", "A", "G", 0.05, FALSE)
print(likes)
prior <- c(1/3,1/3,1/3)
num <-likes * prior
print(num)
post <- num/sum(num)
print(post)
source("CMEECourseWork/Bayesian/Materials/Notebooks/math/Data/functions.R")
mu <- 2
tau <- 0.5
x <- seq(-4,10,0.01)
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,2.0),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3)
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,2.0),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3) # prior
# likelihood
# likelihood
y <- 6
sigma <- 1
points(x=x, y=dnorm(x=y, mean=x, sd=sigma), type="l", lty=2)
# posterior
B <- sigma^2/(sigma^2+tau^2)
postMean <- B*mu + (1-B)*y
postVar <- B*tau^2
points(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type="l", lty=3)
mu <- 2
tau <- 0.5
x <- seq(-4,10,0.01)
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,1.5),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3)
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,1.5),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3) # prior
# likelihood
y <- 6
sigma <- 1
points(x=x, y=dnorm(x=y, mean=x, sd=sigma), type="l", lty=2)
# prior
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,0.6),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3) # prior
# likelihood
points(x=x, y=dnorm(x=y, mean=x, sd=sigma), type="l", lty=2) # likelihood
# posterior
B <- sigma^2/(sigma^2+tau^2)
postMean <- B*mu + (1-B)*y
postVar <- B*tau^2
points(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type="l", lty=3)
# prior
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,1.5),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
mu <- 2
tau <- 0.5
x <- seq(-4,10,0.01)
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,2.0),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3)
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,2.0),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3) # prior
# likelihood
y <- 6
sigma <- 1
points(x=x, y=dnorm(x=y, mean=x, sd=sigma), type="l", lty=2)
# prior
plot(x=x, dnorm(x=x, mean=mu, sd=tau), ylim=c(0,2.0),
type="l", lty=1, ylab="Density", xlab=expression(theta), main="")
legend(x="topleft", legend=c(expression(pi(theta)),
expression(f(y~"|"~theta)), expression(p(theta~"|"~y))), lty=1:3) # prior
# likelihood
points(x=x, y=dnorm(x=y, mean=x, sd=sigma), type="l", lty=2) # likelihood
# posterior
B <- sigma^2/(sigma^2+tau^2)
postMean <- B*mu + (1-B)*y
postVar <- B*tau^2
points(x=x, y=dnorm(x=x, mean=postMean, sd=sqrt(postVar)), type="l", lty=3)
o_data <- read.csv("../Data/LogisticGrowthData.csv")
setwd CMEECourseWork
getwd
setwd(CMEECourseWork)
setwd(../CMEECourseWork)
setwd(..)
source('~/Desktop/S2/MLE/MLE.R')
sum()
sum(y)
source('~/Desktop/S2/MLE/MLE.R')
source('~/Desktop/S2/MLE/MLE.R')
sum(y)
source('~/Desktop/S2/MLE/MLE.R')
source('~/Desktop/S2/MLE/MLE.R')
x
hist(x)
mean(x)
var(x)
x<1:10
y<dpois(x, size=10, lanmda = 2)
x<1:10
y<dpois(x, lanmbda = 2, log =false)
y<dpois(x, lambda = 2, log =false)
y<dpois(x, lambda = 2, log = FALSE)
x<1:10
y<dpois(x, lambda = 2, log = FALSE)
x<- 1:10
y<- dpois(x, lambda = 2, log = FALSE)
plot(x, y, pch=16, ylab = 'pmf', xlab = 'outcome')
x<- seq(1, 10, 1)
y <-dexp(x, lambda = 2)
plot(x, y, pch=16, ylab = 'pdf', xlab = 'outcome')
x<- seq(1, 10, 1)
# dExp(x, scale = 1, params = list(scale = 1), ...), scale(lambda) = scale parameter, called rate in other packages.
y <-dExp(x, scale = 2, params = list(scale = 2))
plot(x, y, pch=16, ylab = 'pdf', xlab = 'outcome')
x<- seq(1, 10, 1)
# dExp(x, scale = 1, params = list(scale = 1), ...), scale(lambda) = scale parameter, called rate in other packages.
y <-dExp(x, scale = 2, params = list(scale = 2))
plot(x, y, pch=16, ylab = 'pdf', xlab = 'outcome')
x<- seq(1, 10, 1)
# dExp(x, scale = 1, params = list(scale = 1), ...), scale(lambda) = scale parameter, called rate in other packages.
y <-dexp(x, scale = 2, params = list(scale = 2))
plot(x, y, pch=16, ylab = 'pdf', xlab = 'outcome')
x<- seq(1, 10, 0.01)
# y_dexp <- dexp(x_dexp, rate = 5)
y <- dexp(x, rate = 2)
plot(x, y, pch=16, ylab = 'pdf', xlab = 'outcome')
pnorm(3)-prnorm(2)
pnorm(3)
pnorm(3)-prnorm(2)
pnorm(3) - pnorm(2)
y<-rnbinom(1000, 1, 0.2)
hist(y)
y<-rnbinom(30*1000, 1, 0.2)
# PUT THEM IN A 1000-BY-30 MATRIX
y.matrix<-matrix(y, nr=1000, nc=30)
# WE THEN CALCULATE THE MEAN FOR EACH ROW
# SO WE HAVE 1000 OF ROW AVERAGES,
# AND WE PLOT THE HISTOGRAM OF THESE AVERAGES
y.row.mean<-apply(y.matrix, 1, mean)
hist(y.row.mean)
cd
getwd
install.packages("xlsx")
rm(list = ls())
graphics.off()
install.packages(c("broom", "car", "dplyr", "ggplot2", "gridExtra", "modelr", "MuMIn", "nls.multstart", "patchwork", "plyr", "progress", "purrr", "Rmisc", "sjstats", "tidyr", "tidyverse"))
remotes::install_github("padpadpadpad/rTPC", build_vignettes = TRUE)
install.packages("remotes")
remotes::install_github("padpadpadpad/rTPC", build_vignettes = TRUE)
remotes::install_github("padpadpadpad/rTPC", build_vignettes = TRUE)
install.packages("nls.multistart")
install.packages(devtools)
install.packages("devtools")
devtools::install_github("padpadpadpad/nls.multstart")
devtools::install_github("padpadpadpad/nls.multstart")
install.packages("devtools")
install.packages("devtools")
devtools::install_github("padpadpadpad/nls.multstart")
remotes::install_github("padpadpadpad/rTPC", build_vignettes = TRUE)
remotes::install_github("padpadpadpad/rTPC", build_vignettes = F)
remotes::install_github("padpadpadpad/rTPC", build_vignettes = F)
emotes::install_github("padpadpadpad/rTPC", build_vignettes = F)
remotes::install_github("padpadpadpad/rTPC", build_vignettes = F)
remotes::install_github("padpadpadpad/rTPC", build_vignettes = TRUE)
#################################################
rm(list = ls())
graphics.off()
# getwd()
setwd("CMEECourseWork/Mainproject/code/")
Vec <- read.csv("../data/VecTraits.csv", header=T)
cera<- read.csv("../data/ceramai_digitised_dataset.csv", header=T)
View(Vec)
